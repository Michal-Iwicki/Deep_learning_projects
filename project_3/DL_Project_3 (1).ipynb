{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhj9zaIDcnZT",
        "outputId": "2d7e425b-c463-457c-bd23-34db3495643d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Seed:  999\n"
          ]
        }
      ],
      "source": [
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torcheval.metrics import FrechetInceptionDistance\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WvgAJtQuuUa_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Parametry\n",
        "dataroot = \"data\"  # katalog z podfolderami obrazów kotów\n",
        "image_size = 64\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "lr = 2e-4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # RGB → (0.5, 0.5, 0.5) jeśli kolor\n",
        "])\n",
        "\n",
        "dataset = dset.ImageFolder(root=dataroot, transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "Ep5n_hPtu51M",
        "outputId": "83648689-95c5-43a0-8ea3-6c10c4aba957"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'workers' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dset\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39mdataroot,\n\u001b[0;32m      4\u001b[0m                            transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      5\u001b[0m                                transforms\u001b[38;5;241m.\u001b[39mResize(image_size),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m                                transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)),\n\u001b[0;32m      9\u001b[0m                            ]))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create the dataloader\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[43mworkers\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Decide which device we want to run on\u001b[39;00m\n\u001b[0;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m ngpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'workers' is not defined"
          ]
        }
      ],
      "source": [
        "# We can use an image folder dataset the way we have it setup.\n",
        "# Create the dataset\n",
        "dataset = dset.ImageFolder(root=dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "# Plot some training images\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Parametry\n",
        "dataroot = \"data/cats\"  # katalog z podfolderami obrazów kotów\n",
        "image_size = 64\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "lr = 2e-4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.use_deterministic_algorithms(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RhQGM_E1v9iR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels=3, hidden_channels=128, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_channels=128, out_channels=3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, hidden_channels, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(hidden_channels, hidden_channels, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, out_channels, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings=512, embedding_dim=64, commitment_cost=0.25):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.commitment_cost = commitment_cost\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z_flattened = z.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
        "        distances = (z_flattened.pow(2).sum(1, keepdim=True)\n",
        "                     - 2 * z_flattened @ self.embedding.weight.t()\n",
        "                     + self.embedding.weight.pow(2).sum(1))\n",
        "\n",
        "        encoding_indices = torch.argmin(distances, dim=1)\n",
        "        encodings = F.one_hot(encoding_indices, self.num_embeddings).float()\n",
        "\n",
        "        quantized = encodings @ self.embedding.weight\n",
        "        quantized = quantized.view(z.shape).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), z)\n",
        "        q_latent_loss = F.mse_loss(quantized, z.detach())\n",
        "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "        quantized = z + (quantized - z).detach()\n",
        "\n",
        "        return quantized, loss\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.quantizer = VectorQuantizer()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        quantized, vq_loss = self.quantizer(z)\n",
        "        x_recon = self.decoder(quantized)\n",
        "        return x_recon, vq_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jSbEJmT_wH2p"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m x_recon, vq_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(x_recon, x)\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m vq_loss\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[11], line 73\u001b[0m, in \u001b[0;36mVQVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     72\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 73\u001b[0m     quantized, vq_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(quantized)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_recon, vq_loss\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[11], line 47\u001b[0m, in \u001b[0;36mVectorQuantizer.forward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m     45\u001b[0m     z_flattened \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim)\n\u001b[0;32m     46\u001b[0m     distances \u001b[38;5;241m=\u001b[39m (z_flattened\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 47\u001b[0m                  \u001b[38;5;241m-\u001b[39m \u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz_flattened\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m                  \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     50\u001b[0m     encoding_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmin(distances, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     51\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(encoding_indices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_embeddings)\u001b[38;5;241m.\u001b[39mfloat()\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility"
          ]
        }
      ],
      "source": [
        "model = VQVAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for data in dataloader:\n",
        "        x, _ = data\n",
        "        x = x.to(device)\n",
        "\n",
        "        x_recon, vq_loss = model(x)\n",
        "        recon_loss = F.mse_loss(x_recon, x)\n",
        "        loss = recon_loss + vq_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtJy5G5RwJ8A",
        "outputId": "cca7c860-c71c-4ab7-b794-56da2f781ee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training Loop...\n",
            "[0/30][0/234]\tLoss_D: 1.8398\tLoss_G: 1.9405\tD(x): 0.2928\tD(G(z)): 0.2715 / 0.1754\n",
            "[0/30][50/234]\tLoss_D: 0.0545\tLoss_G: 4.7530\tD(x): 0.9772\tD(G(z)): 0.0166 / 0.0135\n",
            "[0/30][100/234]\tLoss_D: 0.4649\tLoss_G: 4.4646\tD(x): 0.8428\tD(G(z)): 0.2098 / 0.0171\n",
            "[0/30][150/234]\tLoss_D: 0.5293\tLoss_G: 2.7877\tD(x): 0.8531\tD(G(z)): 0.2588 / 0.0883\n",
            "[0/30][200/234]\tLoss_D: 0.2516\tLoss_G: 6.4612\tD(x): 0.9113\tD(G(z)): 0.1287 / 0.0024\n",
            "[1/30][0/234]\tLoss_D: 0.5019\tLoss_G: 9.0162\tD(x): 0.9631\tD(G(z)): 0.3260 / 0.0003\n",
            "[1/30][50/234]\tLoss_D: 1.1810\tLoss_G: 5.1976\tD(x): 0.4514\tD(G(z)): 0.0009 / 0.0186\n",
            "[1/30][100/234]\tLoss_D: 0.6239\tLoss_G: 3.6995\tD(x): 0.7843\tD(G(z)): 0.2212 / 0.0491\n",
            "[1/30][150/234]\tLoss_D: 0.5459\tLoss_G: 5.2121\tD(x): 0.6600\tD(G(z)): 0.0243 / 0.0138\n",
            "[1/30][200/234]\tLoss_D: 0.4852\tLoss_G: 5.6047\tD(x): 0.8836\tD(G(z)): 0.2530 / 0.0082\n",
            "[2/30][0/234]\tLoss_D: 0.8208\tLoss_G: 7.4122\tD(x): 0.8923\tD(G(z)): 0.4202 / 0.0031\n",
            "[2/30][50/234]\tLoss_D: 1.0545\tLoss_G: 3.0395\tD(x): 0.4880\tD(G(z)): 0.0335 / 0.0733\n",
            "[2/30][100/234]\tLoss_D: 0.9860\tLoss_G: 5.7446\tD(x): 0.5291\tD(G(z)): 0.0045 / 0.0179\n",
            "[2/30][150/234]\tLoss_D: 0.6484\tLoss_G: 4.1909\tD(x): 0.7724\tD(G(z)): 0.2534 / 0.0324\n",
            "[2/30][200/234]\tLoss_D: 0.7593\tLoss_G: 4.4009\tD(x): 0.7552\tD(G(z)): 0.2828 / 0.0251\n",
            "[3/30][0/234]\tLoss_D: 1.4345\tLoss_G: 6.1816\tD(x): 0.9411\tD(G(z)): 0.5391 / 0.0198\n",
            "[3/30][50/234]\tLoss_D: 0.4477\tLoss_G: 4.1420\tD(x): 0.7400\tD(G(z)): 0.0679 / 0.0371\n",
            "[3/30][100/234]\tLoss_D: 0.6310\tLoss_G: 4.3103\tD(x): 0.7988\tD(G(z)): 0.2506 / 0.0364\n",
            "[3/30][150/234]\tLoss_D: 1.0510\tLoss_G: 2.9497\tD(x): 0.5116\tD(G(z)): 0.0557 / 0.1085\n",
            "[3/30][200/234]\tLoss_D: 0.5686\tLoss_G: 3.8716\tD(x): 0.7419\tD(G(z)): 0.1294 / 0.0532\n",
            "[4/30][0/234]\tLoss_D: 0.8020\tLoss_G: 4.8286\tD(x): 0.7444\tD(G(z)): 0.2895 / 0.0306\n",
            "[4/30][50/234]\tLoss_D: 0.6552\tLoss_G: 3.2470\tD(x): 0.7377\tD(G(z)): 0.1944 / 0.0832\n",
            "[4/30][100/234]\tLoss_D: 0.8701\tLoss_G: 3.7380\tD(x): 0.5516\tD(G(z)): 0.0325 / 0.0588\n",
            "[4/30][150/234]\tLoss_D: 0.6896\tLoss_G: 4.9596\tD(x): 0.8252\tD(G(z)): 0.2929 / 0.0181\n",
            "[4/30][200/234]\tLoss_D: 0.6655\tLoss_G: 3.7022\tD(x): 0.7219\tD(G(z)): 0.1703 / 0.0636\n",
            "[5/30][0/234]\tLoss_D: 0.7311\tLoss_G: 2.2973\tD(x): 0.6028\tD(G(z)): 0.0411 / 0.1800\n",
            "[5/30][50/234]\tLoss_D: 0.4857\tLoss_G: 5.0373\tD(x): 0.8267\tD(G(z)): 0.1955 / 0.0173\n",
            "[5/30][100/234]\tLoss_D: 1.1946\tLoss_G: 7.9476\tD(x): 0.8903\tD(G(z)): 0.5540 / 0.0020\n",
            "[5/30][150/234]\tLoss_D: 1.1518\tLoss_G: 2.2869\tD(x): 0.4715\tD(G(z)): 0.0666 / 0.1955\n",
            "[5/30][200/234]\tLoss_D: 0.6832\tLoss_G: 5.7839\tD(x): 0.8266\tD(G(z)): 0.2931 / 0.0124\n",
            "[6/30][0/234]\tLoss_D: 0.9222\tLoss_G: 3.9820\tD(x): 0.5436\tD(G(z)): 0.0232 / 0.0693\n",
            "[6/30][50/234]\tLoss_D: 1.1723\tLoss_G: 6.8671\tD(x): 0.8847\tD(G(z)): 0.5728 / 0.0054\n",
            "[6/30][100/234]\tLoss_D: 0.6510\tLoss_G: 3.3731\tD(x): 0.7382\tD(G(z)): 0.2021 / 0.0739\n",
            "[6/30][150/234]\tLoss_D: 0.5764\tLoss_G: 3.3254\tD(x): 0.6938\tD(G(z)): 0.1049 / 0.0753\n",
            "[6/30][200/234]\tLoss_D: 0.7976\tLoss_G: 5.4388\tD(x): 0.8878\tD(G(z)): 0.4105 / 0.0102\n",
            "[7/30][0/234]\tLoss_D: 0.6037\tLoss_G: 3.1376\tD(x): 0.7130\tD(G(z)): 0.1189 / 0.1288\n",
            "[7/30][50/234]\tLoss_D: 0.9292\tLoss_G: 1.2829\tD(x): 0.5040\tD(G(z)): 0.0387 / 0.3618\n",
            "[7/30][100/234]\tLoss_D: 0.6329\tLoss_G: 4.5328\tD(x): 0.8215\tD(G(z)): 0.2773 / 0.0285\n",
            "[7/30][150/234]\tLoss_D: 0.5977\tLoss_G: 5.5451\tD(x): 0.9140\tD(G(z)): 0.3290 / 0.0088\n",
            "[7/30][200/234]\tLoss_D: 0.6265\tLoss_G: 6.1504\tD(x): 0.8643\tD(G(z)): 0.3117 / 0.0065\n",
            "[8/30][0/234]\tLoss_D: 0.5797\tLoss_G: 6.4900\tD(x): 0.9201\tD(G(z)): 0.3413 / 0.0053\n",
            "[8/30][50/234]\tLoss_D: 0.4005\tLoss_G: 3.3937\tD(x): 0.7577\tD(G(z)): 0.0546 / 0.0548\n",
            "[8/30][100/234]\tLoss_D: 0.5023\tLoss_G: 4.9106\tD(x): 0.8690\tD(G(z)): 0.2525 / 0.0190\n",
            "[8/30][150/234]\tLoss_D: 0.4154\tLoss_G: 3.4698\tD(x): 0.7912\tD(G(z)): 0.1176 / 0.0585\n",
            "[8/30][200/234]\tLoss_D: 0.3562\tLoss_G: 4.0043\tD(x): 0.8788\tD(G(z)): 0.1689 / 0.0351\n",
            "[9/30][0/234]\tLoss_D: 0.6197\tLoss_G: 3.1676\tD(x): 0.8059\tD(G(z)): 0.2308 / 0.0882\n",
            "[9/30][50/234]\tLoss_D: 0.4413\tLoss_G: 3.2246\tD(x): 0.7918\tD(G(z)): 0.1248 / 0.0763\n",
            "[9/30][100/234]\tLoss_D: 1.0683\tLoss_G: 6.4910\tD(x): 0.9113\tD(G(z)): 0.5422 / 0.0047\n",
            "[9/30][150/234]\tLoss_D: 0.3816\tLoss_G: 4.0213\tD(x): 0.8633\tD(G(z)): 0.1764 / 0.0324\n",
            "[9/30][200/234]\tLoss_D: 0.3972\tLoss_G: 4.1459\tD(x): 0.7826\tD(G(z)): 0.0729 / 0.0424\n",
            "[10/30][0/234]\tLoss_D: 0.4304\tLoss_G: 3.9614\tD(x): 0.8723\tD(G(z)): 0.2131 / 0.0341\n",
            "[10/30][50/234]\tLoss_D: 0.6996\tLoss_G: 2.7676\tD(x): 0.6021\tD(G(z)): 0.0388 / 0.1086\n",
            "[10/30][100/234]\tLoss_D: 0.4992\tLoss_G: 2.8737\tD(x): 0.7296\tD(G(z)): 0.1014 / 0.1035\n",
            "[10/30][150/234]\tLoss_D: 0.4564\tLoss_G: 3.3236\tD(x): 0.7317\tD(G(z)): 0.0642 / 0.0789\n",
            "[10/30][200/234]\tLoss_D: 0.6054\tLoss_G: 2.0034\tD(x): 0.6264\tD(G(z)): 0.0253 / 0.2017\n",
            "[11/30][0/234]\tLoss_D: 1.0980\tLoss_G: 2.4137\tD(x): 0.4746\tD(G(z)): 0.0262 / 0.2326\n",
            "[11/30][50/234]\tLoss_D: 0.5855\tLoss_G: 5.2245\tD(x): 0.9283\tD(G(z)): 0.3464 / 0.0132\n",
            "[11/30][100/234]\tLoss_D: 0.4145\tLoss_G: 4.5056\tD(x): 0.9083\tD(G(z)): 0.2430 / 0.0193\n",
            "[11/30][150/234]\tLoss_D: 0.4246\tLoss_G: 3.7564\tD(x): 0.8099\tD(G(z)): 0.1412 / 0.0442\n",
            "[11/30][200/234]\tLoss_D: 0.5034\tLoss_G: 4.2604\tD(x): 0.8142\tD(G(z)): 0.2046 / 0.0253\n",
            "[12/30][0/234]\tLoss_D: 0.4252\tLoss_G: 4.1152\tD(x): 0.8440\tD(G(z)): 0.1770 / 0.0353\n",
            "[12/30][50/234]\tLoss_D: 0.3676\tLoss_G: 3.9975\tD(x): 0.8223\tD(G(z)): 0.1194 / 0.0332\n",
            "[12/30][100/234]\tLoss_D: 0.4577\tLoss_G: 2.9055\tD(x): 0.7582\tD(G(z)): 0.1072 / 0.0814\n",
            "[12/30][150/234]\tLoss_D: 0.4174\tLoss_G: 4.1624\tD(x): 0.8463\tD(G(z)): 0.1802 / 0.0266\n",
            "[12/30][200/234]\tLoss_D: 1.0264\tLoss_G: 6.3964\tD(x): 0.9679\tD(G(z)): 0.5408 / 0.0059\n",
            "[13/30][0/234]\tLoss_D: 0.4655\tLoss_G: 4.1565\tD(x): 0.8133\tD(G(z)): 0.1676 / 0.0281\n",
            "[13/30][50/234]\tLoss_D: 0.4336\tLoss_G: 5.6427\tD(x): 0.9411\tD(G(z)): 0.2702 / 0.0072\n",
            "[13/30][100/234]\tLoss_D: 0.4571\tLoss_G: 3.5328\tD(x): 0.8165\tD(G(z)): 0.1771 / 0.0481\n",
            "[13/30][150/234]\tLoss_D: 0.5895\tLoss_G: 5.1818\tD(x): 0.9293\tD(G(z)): 0.3381 / 0.0114\n",
            "[13/30][200/234]\tLoss_D: 0.7558\tLoss_G: 1.9786\tD(x): 0.5946\tD(G(z)): 0.0946 / 0.2130\n",
            "[14/30][0/234]\tLoss_D: 0.4330\tLoss_G: 3.0187\tD(x): 0.7435\tD(G(z)): 0.0813 / 0.0894\n",
            "[14/30][50/234]\tLoss_D: 0.5163\tLoss_G: 4.5253\tD(x): 0.8862\tD(G(z)): 0.2714 / 0.0257\n",
            "[14/30][100/234]\tLoss_D: 0.5007\tLoss_G: 2.7335\tD(x): 0.7528\tD(G(z)): 0.1176 / 0.1106\n",
            "[14/30][150/234]\tLoss_D: 0.5985\tLoss_G: 3.1256\tD(x): 0.7256\tD(G(z)): 0.1574 / 0.0804\n",
            "[14/30][200/234]\tLoss_D: 1.0308\tLoss_G: 5.5094\tD(x): 0.9431\tD(G(z)): 0.5524 / 0.0096\n",
            "[15/30][0/234]\tLoss_D: 0.8183\tLoss_G: 2.1654\tD(x): 0.5822\tD(G(z)): 0.0741 / 0.1736\n",
            "[15/30][50/234]\tLoss_D: 0.5453\tLoss_G: 4.5992\tD(x): 0.9053\tD(G(z)): 0.3110 / 0.0195\n",
            "[15/30][100/234]\tLoss_D: 0.8108\tLoss_G: 1.4116\tD(x): 0.5573\tD(G(z)): 0.0828 / 0.3226\n",
            "[15/30][150/234]\tLoss_D: 1.0493\tLoss_G: 0.9741\tD(x): 0.4572\tD(G(z)): 0.0535 / 0.4582\n",
            "[15/30][200/234]\tLoss_D: 0.5620\tLoss_G: 3.7913\tD(x): 0.8059\tD(G(z)): 0.2329 / 0.0402\n",
            "[16/30][0/234]\tLoss_D: 0.6412\tLoss_G: 4.3873\tD(x): 0.9124\tD(G(z)): 0.3664 / 0.0233\n",
            "[16/30][50/234]\tLoss_D: 0.4042\tLoss_G: 2.9237\tD(x): 0.7484\tD(G(z)): 0.0592 / 0.0894\n",
            "[16/30][100/234]\tLoss_D: 0.6195\tLoss_G: 2.3761\tD(x): 0.6198\tD(G(z)): 0.0522 / 0.1466\n",
            "[16/30][150/234]\tLoss_D: 0.7455\tLoss_G: 5.5135\tD(x): 0.9457\tD(G(z)): 0.4414 / 0.0094\n",
            "[16/30][200/234]\tLoss_D: 0.4895\tLoss_G: 3.2616\tD(x): 0.7875\tD(G(z)): 0.1765 / 0.0565\n",
            "[17/30][0/234]\tLoss_D: 1.4241\tLoss_G: 8.9544\tD(x): 0.9664\tD(G(z)): 0.6810 / 0.0008\n",
            "[17/30][50/234]\tLoss_D: 0.5674\tLoss_G: 3.9953\tD(x): 0.8037\tD(G(z)): 0.2369 / 0.0355\n",
            "[17/30][100/234]\tLoss_D: 0.5031\tLoss_G: 3.4443\tD(x): 0.8290\tD(G(z)): 0.2134 / 0.0599\n",
            "[17/30][150/234]\tLoss_D: 0.4148\tLoss_G: 3.3171\tD(x): 0.8137\tD(G(z)): 0.1524 / 0.0624\n",
            "[17/30][200/234]\tLoss_D: 0.4595\tLoss_G: 3.1592\tD(x): 0.8204\tD(G(z)): 0.1860 / 0.0708\n",
            "[18/30][0/234]\tLoss_D: 2.4498\tLoss_G: 6.8877\tD(x): 0.9781\tD(G(z)): 0.8250 / 0.0067\n",
            "[18/30][50/234]\tLoss_D: 1.2503\tLoss_G: 5.3305\tD(x): 0.9307\tD(G(z)): 0.6198 / 0.0135\n",
            "[18/30][100/234]\tLoss_D: 0.8982\tLoss_G: 1.6460\tD(x): 0.5006\tD(G(z)): 0.0268 / 0.2542\n",
            "[18/30][150/234]\tLoss_D: 0.3954\tLoss_G: 2.7222\tD(x): 0.7779\tD(G(z)): 0.0818 / 0.1165\n",
            "[18/30][200/234]\tLoss_D: 0.3279\tLoss_G: 4.0326\tD(x): 0.9271\tD(G(z)): 0.1938 / 0.0330\n",
            "[19/30][0/234]\tLoss_D: 0.4276\tLoss_G: 3.0128\tD(x): 0.7484\tD(G(z)): 0.0853 / 0.1007\n",
            "[19/30][50/234]\tLoss_D: 0.9473\tLoss_G: 7.1784\tD(x): 0.9541\tD(G(z)): 0.5212 / 0.0017\n",
            "[19/30][100/234]\tLoss_D: 0.4745\tLoss_G: 4.4187\tD(x): 0.8631\tD(G(z)): 0.2408 / 0.0211\n",
            "[19/30][150/234]\tLoss_D: 0.3930\tLoss_G: 3.4415\tD(x): 0.7578\tD(G(z)): 0.0555 / 0.0618\n",
            "[19/30][200/234]\tLoss_D: 0.3280\tLoss_G: 3.2858\tD(x): 0.8429\tD(G(z)): 0.1191 / 0.0613\n",
            "[20/30][0/234]\tLoss_D: 0.8393\tLoss_G: 8.5594\tD(x): 0.9330\tD(G(z)): 0.4595 / 0.0005\n",
            "[20/30][50/234]\tLoss_D: 0.3549\tLoss_G: 4.0035\tD(x): 0.8593\tD(G(z)): 0.1449 / 0.0320\n",
            "[20/30][100/234]\tLoss_D: 0.5158\tLoss_G: 4.6195\tD(x): 0.9061\tD(G(z)): 0.2858 / 0.0196\n",
            "[20/30][150/234]\tLoss_D: 0.5322\tLoss_G: 6.1512\tD(x): 0.9053\tD(G(z)): 0.3033 / 0.0041\n",
            "[20/30][200/234]\tLoss_D: 0.3621\tLoss_G: 3.7071\tD(x): 0.8514\tD(G(z)): 0.1538 / 0.0402\n",
            "[21/30][0/234]\tLoss_D: 0.3242\tLoss_G: 3.6595\tD(x): 0.8560\tD(G(z)): 0.1248 / 0.0481\n",
            "[21/30][50/234]\tLoss_D: 0.4867\tLoss_G: 3.4921\tD(x): 0.7971\tD(G(z)): 0.1723 / 0.0501\n",
            "[21/30][100/234]\tLoss_D: 0.7581\tLoss_G: 4.8893\tD(x): 0.9715\tD(G(z)): 0.4290 / 0.0184\n",
            "[21/30][150/234]\tLoss_D: 0.2824\tLoss_G: 3.5764\tD(x): 0.8750\tD(G(z)): 0.1112 / 0.0506\n",
            "[21/30][200/234]\tLoss_D: 0.3236\tLoss_G: 3.6008\tD(x): 0.8190\tD(G(z)): 0.0865 / 0.0442\n",
            "[22/30][0/234]\tLoss_D: 0.4882\tLoss_G: 5.6624\tD(x): 0.9210\tD(G(z)): 0.2814 / 0.0068\n",
            "[22/30][50/234]\tLoss_D: 0.3236\tLoss_G: 3.7837\tD(x): 0.8022\tD(G(z)): 0.0695 / 0.0437\n",
            "[22/30][100/234]\tLoss_D: 0.6739\tLoss_G: 6.2463\tD(x): 0.9511\tD(G(z)): 0.4032 / 0.0052\n",
            "[22/30][150/234]\tLoss_D: 0.4873\tLoss_G: 2.4072\tD(x): 0.7024\tD(G(z)): 0.0484 / 0.1339\n",
            "[22/30][200/234]\tLoss_D: 0.4597\tLoss_G: 2.3769\tD(x): 0.7201\tD(G(z)): 0.0665 / 0.1374\n",
            "[23/30][0/234]\tLoss_D: 1.0930\tLoss_G: 6.2423\tD(x): 0.9400\tD(G(z)): 0.5416 / 0.0059\n",
            "[23/30][50/234]\tLoss_D: 0.6673\tLoss_G: 5.2112\tD(x): 0.9759\tD(G(z)): 0.4005 / 0.0183\n",
            "[23/30][100/234]\tLoss_D: 0.4613\tLoss_G: 5.0134\tD(x): 0.9393\tD(G(z)): 0.2890 / 0.0122\n",
            "[23/30][150/234]\tLoss_D: 0.4953\tLoss_G: 1.9604\tD(x): 0.7055\tD(G(z)): 0.0746 / 0.2073\n",
            "[23/30][200/234]\tLoss_D: 0.3018\tLoss_G: 3.5629\tD(x): 0.9204\tD(G(z)): 0.1670 / 0.0460\n",
            "[24/30][0/234]\tLoss_D: 4.2888\tLoss_G: 7.6908\tD(x): 0.9934\tD(G(z)): 0.9616 / 0.0059\n",
            "[24/30][50/234]\tLoss_D: 0.3578\tLoss_G: 2.8876\tD(x): 0.8155\tD(G(z)): 0.1125 / 0.0840\n",
            "[24/30][100/234]\tLoss_D: 0.3778\tLoss_G: 2.4246\tD(x): 0.7674\tD(G(z)): 0.0670 / 0.1292\n",
            "[24/30][150/234]\tLoss_D: 0.5670\tLoss_G: 2.5636\tD(x): 0.6820\tD(G(z)): 0.0827 / 0.1316\n",
            "[24/30][200/234]\tLoss_D: 0.4352\tLoss_G: 4.5318\tD(x): 0.8434\tD(G(z)): 0.1952 / 0.0230\n",
            "[25/30][0/234]\tLoss_D: 0.2201\tLoss_G: 4.4608\tD(x): 0.9291\tD(G(z)): 0.1221 / 0.0185\n",
            "[25/30][50/234]\tLoss_D: 0.8283\tLoss_G: 1.9108\tD(x): 0.5196\tD(G(z)): 0.0154 / 0.2218\n",
            "[25/30][100/234]\tLoss_D: 0.2978\tLoss_G: 3.9917\tD(x): 0.9101\tD(G(z)): 0.1626 / 0.0304\n",
            "[25/30][150/234]\tLoss_D: 0.4847\tLoss_G: 3.8468\tD(x): 0.8590\tD(G(z)): 0.2361 / 0.0342\n",
            "[25/30][200/234]\tLoss_D: 0.5282\tLoss_G: 5.1185\tD(x): 0.9631\tD(G(z)): 0.3461 / 0.0117\n",
            "[26/30][0/234]\tLoss_D: 1.2141\tLoss_G: 1.0750\tD(x): 0.4267\tD(G(z)): 0.0320 / 0.4514\n",
            "[26/30][50/234]\tLoss_D: 0.2417\tLoss_G: 3.4452\tD(x): 0.8404\tD(G(z)): 0.0526 / 0.0533\n",
            "[26/30][100/234]\tLoss_D: 2.4073\tLoss_G: 0.7300\tD(x): 0.1703\tD(G(z)): 0.0032 / 0.5713\n",
            "[26/30][150/234]\tLoss_D: 0.4188\tLoss_G: 3.7278\tD(x): 0.8616\tD(G(z)): 0.2038 / 0.0391\n",
            "[26/30][200/234]\tLoss_D: 0.4064\tLoss_G: 2.9245\tD(x): 0.7803\tD(G(z)): 0.1006 / 0.0829\n",
            "[27/30][0/234]\tLoss_D: 0.3271\tLoss_G: 3.7346\tD(x): 0.8254\tD(G(z)): 0.0976 / 0.0407\n",
            "[27/30][50/234]\tLoss_D: 0.3780\tLoss_G: 3.8995\tD(x): 0.8893\tD(G(z)): 0.1863 / 0.0392\n",
            "[27/30][100/234]\tLoss_D: 0.3124\tLoss_G: 3.6024\tD(x): 0.9068\tD(G(z)): 0.1683 / 0.0412\n",
            "[27/30][150/234]\tLoss_D: 0.3611\tLoss_G: 3.3588\tD(x): 0.7996\tD(G(z)): 0.0725 / 0.0528\n",
            "[27/30][200/234]\tLoss_D: 0.7569\tLoss_G: 4.6466\tD(x): 0.9469\tD(G(z)): 0.4398 / 0.0202\n",
            "[28/30][0/234]\tLoss_D: 1.0648\tLoss_G: 7.3254\tD(x): 0.9915\tD(G(z)): 0.5527 / 0.0018\n",
            "[28/30][50/234]\tLoss_D: 0.2951\tLoss_G: 3.5140\tD(x): 0.8803\tD(G(z)): 0.1324 / 0.0518\n",
            "[28/30][100/234]\tLoss_D: 0.9529\tLoss_G: 7.9737\tD(x): 0.9602\tD(G(z)): 0.5372 / 0.0008\n",
            "[28/30][150/234]\tLoss_D: 0.5521\tLoss_G: 1.6656\tD(x): 0.6624\tD(G(z)): 0.0625 / 0.2736\n",
            "[28/30][200/234]\tLoss_D: 0.3207\tLoss_G: 3.9296\tD(x): 0.9045\tD(G(z)): 0.1774 / 0.0296\n",
            "[29/30][0/234]\tLoss_D: 0.3376\tLoss_G: 4.8408\tD(x): 0.9785\tD(G(z)): 0.2419 / 0.0136\n",
            "[29/30][50/234]\tLoss_D: 1.1786\tLoss_G: 0.2694\tD(x): 0.4187\tD(G(z)): 0.0112 / 0.8085\n",
            "[29/30][100/234]\tLoss_D: 0.2000\tLoss_G: 4.7837\tD(x): 0.9378\tD(G(z)): 0.1071 / 0.0142\n",
            "[29/30][150/234]\tLoss_D: 0.2639\tLoss_G: 3.5256\tD(x): 0.9216\tD(G(z)): 0.1495 / 0.0457\n",
            "[29/30][200/234]\tLoss_D: 0.3132\tLoss_G: 4.9094\tD(x): 0.9187\tD(G(z)): 0.1770 / 0.0130\n"
          ]
        }
      ],
      "source": [
        "# Załaduj wytrenowany model\n",
        "# model = VQVAE().to(device)\n",
        "# model.load_state_dict(torch.load(\"vqvae_cat.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Generuj losowe kody z codebooku\n",
        "def generate_images(model, num_images=16):\n",
        "    embedding_dim = model.quantizer.embedding_dim\n",
        "    num_embeddings = model.quantizer.num_embeddings\n",
        "    spatial_dim = (16, 16)  # zależnie od z→ downsampling 64x64 → 16x16\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Wylosuj indeksy\n",
        "        random_indices = torch.randint(0, num_embeddings, (num_images, *spatial_dim)).to(device)\n",
        "        quantized = model.quantizer.embedding(random_indices)\n",
        "        quantized = quantized.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
        "        recon = model.decoder(quantized)\n",
        "\n",
        "        grid = vutils.make_grid(recon, nrow=4, normalize=True, scale_each=True)\n",
        "        plt.figure(figsize=(8,8))\n",
        "        plt.axis('off')\n",
        "        plt.title('Wygenerowane koty')\n",
        "        plt.imshow(np.transpose(grid.cpu(), (1,2,0)))\n",
        "        plt.show()\n",
        "\n",
        "generate_images(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JVR743RsMtf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
