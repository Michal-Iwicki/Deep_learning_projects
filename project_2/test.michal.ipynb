{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c51fb960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 000 files proccessed\n",
      "10 000 files proccessed\n",
      "10 000 files proccessed\n",
      "10 000 files proccessed\n",
      "10 000 files proccessed\n",
      "10 000 files proccessed\n"
     ]
    }
   ],
   "source": [
    "from new_loader import preprocess_and_save_audio_in_tensors\n",
    "preprocess_and_save_audio_in_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01cefd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 000 files proccessed\n",
      "10 000 files proccessed\n",
      "10 000 files proccessed\n",
      "10 000 files proccessed\n",
      "10 000 files proccessed\n",
      "10 000 files proccessed\n"
     ]
    }
   ],
   "source": [
    "from new_loader import preprocess_and_save_audio_in_tensors\n",
    "preprocess_and_save_audio_in_tensors(denoise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7029e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import efficientnet_b0\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_loading import TorchTensorFolderDataset\n",
    "def get_loader(data_size:str=\"\", target_data:str=\"train\", batch_size:int=16):\n",
    "    # path_to_raw = os.path.join(os.getcwd(), \"data\", \"preprocessed\", data_size, \"raw\", target_data)\n",
    "    path_to_mel = os.path.join(os.getcwd(), \"data\", \"preprocessed\", data_size, \"raw\", target_data)\n",
    "    \n",
    "    # dataset = EnsembleDataset(path_to_raw, path_to_mel)\n",
    "    dataset = TorchTensorFolderDataset(path_to_mel)\n",
    "    # n = len(dataset.labels)\n",
    "    n = len(dataset.class_to_idx)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
    "\n",
    "    return loader, n\n",
    "\n",
    "loader = get_loader()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "812bbbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1095)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "print(max(batch[0][0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79ad4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "1500\n",
      "1751\n"
     ]
    }
   ],
   "source": [
    "from new_loader import generate_dataset_with_optional_augmented_sample\n",
    "generate_dataset_with_optional_augmented_sample(denoise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1d514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class NoiseVoiceClassificationDataset(Dataset):\n",
    "    def __init__(self, folder_noise, folder_voice, seed=42):\n",
    "        self.samples = []\n",
    "\n",
    "        # Wczytaj ścieżki noise i voice\n",
    "        noise_paths = [os.path.join(folder_noise, f) for f in os.listdir(folder_noise) if f.endswith('.pt')]\n",
    "\n",
    "        voice_paths = []\n",
    "        for root, _, files in os.walk(folder_voice):\n",
    "            for f in files:\n",
    "                if f.endswith('.pt'):\n",
    "                    voice_paths.append(os.path.join(root, f))\n",
    "\n",
    "        # Ustal liczbę próbek\n",
    "        num_noise = len(noise_paths)\n",
    "\n",
    "        # Losuj voice z zachowaniem równowagi i deterministyczności\n",
    "        rnd = random.Random(seed)\n",
    "        voice_paths_sampled = rnd.sample(voice_paths, k=min(num_noise, len(voice_paths)))\n",
    "\n",
    "        # Buduj listę próbek: (ścieżka, etykieta)\n",
    "        self.samples.extend([(p, 0) for p in noise_paths])\n",
    "        self.samples.extend([(p, 1) for p in voice_paths_sampled])\n",
    "\n",
    "        # Opcjonalnie przemieszaj wszystkie próbki (zachowując seed)\n",
    "        rnd.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        tensor = torch.load(path,weights_only=False)\n",
    "        return tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_dataloaders(noise_dir, voice_dir, seed=42, batchsize=16):\n",
    "    splits = ['train', 'validation', 'test']\n",
    "    loaders = {}\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "        noise_folder = os.path.join(noise_dir, split)\n",
    "        voice_folder = os.path.join(voice_dir, split)\n",
    "\n",
    "        dataset = NoiseVoiceClassificationDataset(noise_folder, voice_folder, seed=seed + i)\n",
    "\n",
    "        loaders[split] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batchsize,\n",
    "            shuffle=(True if split == 'train' else False)\n",
    "        )\n",
    "\n",
    "    return loaders['train'], loaders['validation'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83b1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dir = r\"C:\\Users\\Michał\\Documents\\Projects\\Deep_learning_projects\\project_2\\data\\preprocessed\\noise\\standard\\raw\"\n",
    "voice_dir = r\"C:\\Users\\Michał\\Documents\\Projects\\Deep_learning_projects\\project_2\\data\\preprocessed\\raw\"\n",
    "train,val,test = load_dataloaders(noise_dir,voice_dir, batchsize = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69806bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_transfomers_implementation import Mel_transformer, RawAudioTransformer, train_transformer\n",
    "train,val,test = load_dataloaders(noise_dir,voice_dir, batchsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d982f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1193 | Train Acc: 0.9805 | Val Acc: 0.9971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0333 | Train Acc: 0.9924 | Val Acc: 0.9986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0128 | Train Acc: 0.9971 | Val Acc: 0.9986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0222 | Train Acc: 0.9952 | Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0120 | Train Acc: 0.9952 | Val Acc: 0.9971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0066 | Train Acc: 0.9986 | Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0015 | Train Acc: 1.0000 | Val Acc: 1.0000\n",
      "Early stopping triggered at epoch 7\n",
      "Best Val Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "spect_model = Mel_transformer(num_classes=30)\n",
    "spect_model = train_transformer(spect_model, train, val, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b03364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0011)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CNN_transfomers_implementation import evaluate_model\n",
    "evaluate_model(spect_model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0baca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
