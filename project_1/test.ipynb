{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class PNGDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for label in os.listdir(root_dir):\n",
    "            label_path = os.path.join(root_dir, label)\n",
    "            if os.path.isdir(label_path):\n",
    "                for file in os.listdir(label_path):\n",
    "                    if file.endswith('.png'):\n",
    "                        self.files.append(os.path.join(label_path, file))\n",
    "                        self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Funkcja do ładowania danych\n",
    "\n",
    "def load_png_images(root_dir, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),          # Konwersja do tensorów\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalizacja\n",
    "    ])\n",
    "    \n",
    "    dataset = PNGDataset(root_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_random_png():\n",
    "\n",
    "    root_dir = r\"C:\\Users\\Micha\\SEM1_projects\\Deep_learning_projects\\project_1\\data\\sample\"\n",
    "    dataloader = load_png_images(root_dir, batch_size=1)  \n",
    "    data_iter = iter(dataloader)\n",
    "    image, label = next(data_iter)\n",
    "\n",
    "    image = image.squeeze(0) \n",
    "    image = image * 0.5 + 0.5  \n",
    "    image = image.permute(1, 2, 0).numpy()  \n",
    "    image = (image * 255).astype(np.uint8) \n",
    "\n",
    "    # Wyświetlenie obrazu\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {label[0]}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_random_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "Zawartość tensora obrazu:\n",
      "[[-0.2235294  -0.30196077 -0.26274508 ... -0.64705884 -0.6313726\n",
      "  -0.20784312]\n",
      " [-0.23921567 -0.3333333  -0.3333333  ... -0.654902   -0.62352943\n",
      "  -0.5921569 ]\n",
      " [-0.11372548  0.02745104  0.05882359 ... -0.60784316 -0.4980392\n",
      "  -0.60784316]\n",
      " ...\n",
      " [-0.2862745   0.5294118   0.77254903 ...  0.8352941   0.81960785\n",
      "   0.70980394]\n",
      " [ 0.04313731  0.77254903  0.79607844 ...  0.8352941   0.7647059\n",
      "   0.6313726 ]\n",
      " [ 0.5137255   0.8117647   0.8117647  ...  0.8039216   0.7411765\n",
      "   0.6627451 ]]\n"
     ]
    }
   ],
   "source": [
    "# Pobierz jeden batch z DataLoadera\n",
    "root_dir = r\"C:\\Users\\Micha\\SEM1_projects\\Deep_learning_projects\\project_1\\data\\test\"\n",
    "dataloader = load_png_images(root_dir, batch_size=1) \n",
    "data_iter = iter(dataloader)\n",
    "image, label = next(data_iter)\n",
    "\n",
    "# Wypisz kształt tensora\n",
    "print(image.shape)  \n",
    "print(\"Zawartość tensora obrazu:\")\n",
    "print(image.squeeze(0).numpy()[0])  # Usunięcie wymiaru batch dla czytelności\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartości piksela (0,0): tensor([[ 0.9843,  0.9922,  0.9922,  ...,  0.9922,  0.9608,  0.9294],\n",
      "        [ 1.0000,  0.9451,  0.9451,  ...,  0.9529,  0.9216,  0.3176],\n",
      "        [ 0.9922,  0.6000,  0.5529,  ...,  0.5686,  0.5686, -0.1843],\n",
      "        ...,\n",
      "        [ 0.9608,  0.4902,  0.3882,  ...,  0.5216,  0.4588, -0.2549],\n",
      "        [ 0.9216,  0.0667, -0.0275,  ...,  0.0667,  0.0275, -0.2941],\n",
      "        [ 0.4588, -0.3412, -0.3333,  ..., -0.2392, -0.2941, -0.2784]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Wartości piksela (0,0):\", image[0, 0, :, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "def calculate_accuracy(loader, model):\n",
    "    model.eval()  # ustawienie modelu w tryb ewaluacji\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # nie śledzimy gradientów podczas testowania\n",
    "        for data, target in loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "class PNGDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        self.label_to_idx = {}\n",
    "        \n",
    "        for idx, label in enumerate(os.listdir(root_dir)):\n",
    "            label_path = os.path.join(root_dir, label)\n",
    "            if os.path.isdir(label_path):\n",
    "                self.label_to_idx[label] = idx\n",
    "                for file in os.listdir(label_path):\n",
    "                    if file.endswith('.png'):\n",
    "                        self.files.append(os.path.join(label_path, file))\n",
    "                        self.labels.append(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Funkcja do ładowania danych\n",
    "\n",
    "def load_png_images(root_dir, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    dataset = PNGDataset(root_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader, len(dataset.label_to_idx)\n",
    "\n",
    "# Definicja prostej sieci CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Funkcja do trenowania modelu\n",
    "def train_model(model, dataloader, num_epochs=10, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, torch.tensor(labels, dtype=torch.long))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            accuracy = calculate_accuracy(dataloader, model)\n",
    "\n",
    "            # Wyświetlanie wyników po każdej epoce\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micha\\AppData\\Local\\Temp\\ipykernel_4332\\2494929351.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(outputs, torch.tensor(labels, dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1737, Accuracy: 60.00%\n",
      "Epoch 1/10, Loss: 0.3473, Accuracy: 59.00%\n",
      "Epoch 1/10, Loss: 0.5212, Accuracy: 57.00%\n",
      "Epoch 1/10, Loss: 0.6887, Accuracy: 50.00%\n",
      "Epoch 2/10, Loss: 0.1665, Accuracy: 50.00%\n",
      "Epoch 2/10, Loss: 0.3304, Accuracy: 50.00%\n",
      "Epoch 2/10, Loss: 0.5162, Accuracy: 50.00%\n",
      "Epoch 2/10, Loss: 0.7026, Accuracy: 62.00%\n",
      "Epoch 3/10, Loss: 0.1562, Accuracy: 63.00%\n",
      "Epoch 3/10, Loss: 0.3177, Accuracy: 67.00%\n",
      "Epoch 3/10, Loss: 0.4919, Accuracy: 65.00%\n",
      "Epoch 3/10, Loss: 0.6684, Accuracy: 66.00%\n",
      "Epoch 4/10, Loss: 0.1595, Accuracy: 63.00%\n",
      "Epoch 4/10, Loss: 0.3203, Accuracy: 65.00%\n",
      "Epoch 4/10, Loss: 0.4940, Accuracy: 64.00%\n",
      "Epoch 4/10, Loss: 0.6486, Accuracy: 66.00%\n",
      "Epoch 5/10, Loss: 0.1558, Accuracy: 62.00%\n",
      "Epoch 5/10, Loss: 0.3206, Accuracy: 66.00%\n",
      "Epoch 5/10, Loss: 0.4824, Accuracy: 68.00%\n",
      "Epoch 5/10, Loss: 0.6542, Accuracy: 73.00%\n",
      "Epoch 6/10, Loss: 0.1515, Accuracy: 68.00%\n",
      "Epoch 6/10, Loss: 0.3064, Accuracy: 67.00%\n",
      "Epoch 6/10, Loss: 0.4722, Accuracy: 66.00%\n",
      "Epoch 6/10, Loss: 0.6037, Accuracy: 65.00%\n",
      "Epoch 7/10, Loss: 0.1512, Accuracy: 63.00%\n",
      "Epoch 7/10, Loss: 0.3317, Accuracy: 67.00%\n",
      "Epoch 7/10, Loss: 0.4821, Accuracy: 70.00%\n",
      "Epoch 7/10, Loss: 0.5891, Accuracy: 71.00%\n",
      "Epoch 8/10, Loss: 0.1368, Accuracy: 73.00%\n",
      "Epoch 8/10, Loss: 0.2921, Accuracy: 73.00%\n",
      "Epoch 8/10, Loss: 0.4369, Accuracy: 73.00%\n",
      "Epoch 8/10, Loss: 0.6039, Accuracy: 69.00%\n",
      "Epoch 9/10, Loss: 0.1475, Accuracy: 65.00%\n",
      "Epoch 9/10, Loss: 0.3116, Accuracy: 65.00%\n",
      "Epoch 9/10, Loss: 0.4584, Accuracy: 70.00%\n",
      "Epoch 9/10, Loss: 0.6103, Accuracy: 73.00%\n",
      "Epoch 10/10, Loss: 0.1547, Accuracy: 76.00%\n",
      "Epoch 10/10, Loss: 0.2806, Accuracy: 77.00%\n",
      "Epoch 10/10, Loss: 0.4415, Accuracy: 75.00%\n",
      "Epoch 10/10, Loss: 0.5723, Accuracy: 75.00%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Ścieżka do katalogu z danymi (podkatalogi to etykiety)\n",
    "root_dir = r\"C:\\Users\\Micha\\SEM1_projects\\Deep_learning_projects\\project_1\\data\\sample\"\n",
    "\n",
    "# Załaduj dane\n",
    "dataloader, num_classes = load_png_images(root_dir, batch_size=32)\n",
    "\n",
    "# Stwórz model\n",
    "model = SimpleCNN(num_classes)\n",
    "\n",
    "# Trenowanie modelu\n",
    "train_model(model, dataloader, num_epochs=10, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
