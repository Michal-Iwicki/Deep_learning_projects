{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac283a61-7b70-4096-9a6a-ab0627ea3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import higher\n",
    "\n",
    "from loader import PNGDataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f7c0ff8-0d2c-4b31-9bfa-6b4c87a7a80c",
   "metadata": {},
   "source": [
    "%pip install higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12cc0a7a-1a14-4bf5-9bd8-fc5018b21f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, embedding_size=64, w1=64, w2=128, w3=256, dropout_rate=0.2, use_bn=True):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, w1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(w1, w2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(w2, w3, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(w1) if use_bn else nn.Identity()\n",
    "        self.bn2 = nn.BatchNorm2d(w2) if use_bn else nn.Identity()\n",
    "        self.bn3 = nn.BatchNorm2d(w3) if use_bn else nn.Identity()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fun = F.relu\n",
    "        \n",
    "        self.fc1 = nn.Linear(w3 * 4 * 4, 128)\n",
    "        self.drop =  nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, embedding_size)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.fun(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.fun(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.fun(self.bn3(self.conv3(x))))\n",
    "        x = self.drop(torch.flatten(x, 1))\n",
    "        x = self.fun(self.fc1(x))\n",
    "        x = self.fc2(self.drop(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bec78aa-cf21-4b39-9cf1-ed910d1eeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMLModel(nn.Module):\n",
    "    def __init__(self, model, n_classes=10):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "    def forward(self, support_set, query_set):        \n",
    "        # Compute support set embeddings\n",
    "        support_embeddings = self.model(support_set)\n",
    "        query_embeddings = self.model(query_set)\n",
    "        \n",
    "        # Compute prototypes (mean of support set embeddings)\n",
    "        prototypes = self.compute_prototypes(support_embeddings)\n",
    "        \n",
    "        # Calculate distances between query samples and prototypes\n",
    "        distances = self.compute_distances(query_embeddings, prototypes)\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def compute_prototypes(self, support_embeddings):\n",
    "        # Reshape support_embeddings and compute class prototypes\n",
    "        support_embeddings = support_embeddings.view(self.n_classes, -1, support_embeddings.size(-1))\n",
    "        prototypes = support_embeddings.mean(dim=1)\n",
    "        \n",
    "        return prototypes\n",
    "    \n",
    "    def compute_distances(self, query_embeddings, prototypes):\n",
    "        # Compute squared Euclidean distance between query embeddings and prototypes\n",
    "        distances = torch.cdist(query_embeddings, prototypes)\n",
    "        \n",
    "        return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d73e8c6-1b84-44d1-9853-a43c4ecae9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotDataset(Dataset):\n",
    "    def __init__(self, dataset, n_classes=10, n_shots=5, n_queries=15, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.n_classes = n_classes\n",
    "        self.n_shots = n_shots\n",
    "        self.n_queries = n_queries\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Group images by their labels\n",
    "        self.class_images = {}\n",
    "        for i in range(len(dataset)):\n",
    "            img, label = dataset[i]\n",
    "            if label not in self.class_images:\n",
    "                self.class_images[label] = []\n",
    "            self.class_images[label].append(img)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000  # Number of episodes\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Randomly sample classes\n",
    "        selected_classes = np.random.choice(list(self.class_images.keys()), self.n_classes, replace=False)\n",
    "        \n",
    "        support_set = []\n",
    "        query_set = []\n",
    "        support_labels = []\n",
    "        query_labels = []\n",
    "        \n",
    "        for label in selected_classes:\n",
    "            images = self.class_images[label]\n",
    "            np.random.shuffle(images)\n",
    "            \n",
    "            # Sample K images for the support set\n",
    "            support_images = images[:self.n_shots]\n",
    "            support_set.extend(support_images)\n",
    "            support_labels.extend([label] * self.n_shots)\n",
    "            \n",
    "            # Sample query set (n_queries)\n",
    "            query_images = images[self.n_shots:self.n_shots + self.n_queries]\n",
    "            query_set.extend(query_images)\n",
    "            query_labels.extend([label] * self.n_queries)\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            support_set = [self.transform(img) for img in support_set]\n",
    "            query_set = [self.transform(img) for img in query_set]\n",
    "        \n",
    "        support_set = torch.stack(support_set)\n",
    "        query_set = torch.stack(query_set)\n",
    "        \n",
    "        return support_set, query_set, torch.tensor(support_labels), torch.tensor(query_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4166c4a2-bcf6-4ce9-9d37-5e71ccd6a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4788952171802521, 0.4722793698310852, 0.43047481775283813],\n",
    "        std=[0.24205632507801056, 0.2382805347442627, 0.25874853134155273]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65492070-7694-4e96-bf1b-e0e33717eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PNGDataset(\"data/sample/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c2575f4-7e42-4e1e-9353-c5d4e3c11db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_dataset = FewShotDataset(train_dataset, n_classes=2, n_shots=5, n_queries=15, transform=transform)\n",
    "train_loader = DataLoader(few_shot_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34e6953-6d30-4ab7-b169-8b5ef2067d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAMLModel(ConvNet(embedding_size=64), n_classes=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f2213ad-942f-40f9-b8f1-d2e31fa2b274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 21.8740\n",
      "Epoch [2/5], Loss: 21.8455\n",
      "Epoch [3/5], Loss: 21.8742\n",
      "Epoch [4/5], Loss: 21.8552\n",
      "Epoch [5/5], Loss: 21.8874\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for support_set, query_set, support_labels, query_labels in train_loader:    \n",
    "        # Define meta-objective for MAML\n",
    "        with higher.innerloop_ctx(model, optimizer, copy_initial_weights=True) as (meta_model, meta_optimizer):\n",
    "            # Loop over episodes\n",
    "            loss = 0.0\n",
    "            for i in range(support_set.size(0)):\n",
    "                support_batch = support_set[i]\n",
    "                query_batch = query_set[i]\n",
    "    \n",
    "                # Forward pass through the model\n",
    "                distances = meta_model(support_batch, query_batch)\n",
    "                \n",
    "                # Calculate loss for this task (using cross entropy)\n",
    "                predicted_labels = torch.argmin(distances, dim=1)\n",
    "                task_loss = F.cross_entropy(distances, query_labels[i])\n",
    "                loss += task_loss\n",
    "    \n",
    "            # Compute gradients and update the model\n",
    "            meta_optimizer.step(loss)\n",
    "    \n",
    "        # Compute the meta-gradient and update the model\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate and perform additional steps as necessary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcd43709-9bf9-42c0-b249-13b588a4005e",
   "metadata": {},
   "source": [
    "?torch.argmin"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2baf896-36c9-4976-b3c4-91a82436ebc6",
   "metadata": {},
   "source": [
    "?torch.nn.functional.cross_entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
